{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11585380,"sourceType":"datasetVersion","datasetId":7264109}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# Special tokens for sequence processing\nSTART_TOKEN = '\\t'\nEND_TOKEN = '\\n'\nPAD_TOKEN = ' '\n\ndef load_dataset(file_path, as_characters=False):\n    \"\"\"\n    Loads data from a TSV file and returns input-output pairs.\n    If as_characters is True, returns lists of characters; otherwise, returns strings.\n    \"\"\"\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = [line.strip().split('\\t') for line in file if line.strip()]\n    # Assuming the format: target \\t input\n    sources = [entry[1] for entry in lines]\n    targets = [entry[0] for entry in lines]\n    if as_characters:\n        sources = [list(text) for text in sources]\n        targets = [list(text) for text in targets]\n    return sources, targets\n\ndef vectorize_sequences(sequences, max_length, char_to_index):\n    \"\"\"\n    Converts sequences of characters to sequences of integer indices, padded to max_length.\n    \"\"\"\n    return np.array([\n        [char_to_index.get(char, char_to_index[PAD_TOKEN]) for char in seq] +\n        [char_to_index[PAD_TOKEN]] * (max_length - len(seq))\n        for seq in sequences\n    ])\n\ndef prepare_data(inputs, max_input_len, input_vocab, targets=None, max_output_len=None, output_vocab=None):\n    \"\"\"\n    Prepares encoder and decoder inputs and targets for training sequence models.\n    \"\"\"\n    encoder_input = vectorize_sequences(inputs, max_input_len, input_vocab)\n\n    decoder_input = None\n    decoder_target = None\n\n    if targets is not None and max_output_len is not None and output_vocab is not None:\n        # Add start and end tokens to targets\n        decoder_input = np.array([\n            [output_vocab[START_TOKEN]] +\n            [output_vocab[char] for char in seq] +\n            [output_vocab[END_TOKEN]] +\n            [output_vocab[PAD_TOKEN]] * (max_output_len - len(seq) - 2)\n            for seq in targets\n        ])\n        decoder_target = np.zeros((decoder_input.shape[0], max_output_len, len(output_vocab)), dtype='float32')\n        for i, seq in enumerate(decoder_input):\n            for t in range(1, len(seq)):\n                decoder_target[i, t-1, seq[t]] = 1.0\n            # Pad the rest with PAD_TOKEN\n            decoder_target[i, t:, output_vocab[PAD_TOKEN]] = 1.0\n\n    return encoder_input, decoder_input, decoder_target\n\ndef build_vocabularies(train_src, train_tgt, val_src, val_tgt):\n    \"\"\"\n    Builds character-level vocabularies for both input and output languages.\n    Returns encoding/decoding dictionaries and max sequence lengths.\n    \"\"\"\n    input_chars = set(char for seq in train_src + val_src for char in seq)\n    output_chars = set(char for seq in train_tgt + val_tgt for char in seq)\n\n    input_vocab = {char: idx for idx, char in enumerate(sorted(input_chars | {PAD_TOKEN}))}\n    input_vocab_dec = sorted(input_vocab, key=lambda k: input_vocab[k])\n    max_input_len = max(len(seq) for seq in train_src + val_src)\n\n    # Output vocab includes special tokens\n    output_vocab = {START_TOKEN: 0, END_TOKEN: 1}\n    idx = 2\n    for char in sorted(output_chars | {PAD_TOKEN}):\n        if char not in output_vocab:\n            output_vocab[char] = idx\n            idx += 1\n    output_vocab_dec = sorted(output_vocab, key=lambda k: output_vocab[k])\n    max_output_len = max(len(seq) for seq in train_tgt + val_tgt) + 2  # +2 for start/end tokens\n\n    print(f\"Training samples: {len(train_src)}\")\n    print(f\"Validation samples: {len(val_src)}\")\n    print(f\"Unique input tokens: {len(input_vocab)}\")\n    print(f\"Unique output tokens: {len(output_vocab)}\")\n    print(f\"Max input length: {max_input_len}\")\n    print(f\"Max output length: {max_output_len}\")\n\n    return input_vocab, input_vocab_dec, output_vocab, output_vocab_dec, max_input_len, max_output_len\n\n# File paths (update as needed)\ntrain_path = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv'\nval_path = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv'\ntest_path = '/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv'\n\n# Load datasets\ntrain_inputs, train_targets = load_dataset(train_path)\nval_inputs, val_targets = load_dataset(val_path)\ntest_inputs, test_targets = load_dataset(test_path)\n\n# Build vocabularies and get sequence lengths\n(input_vocab, input_vocab_dec, output_vocab, output_vocab_dec,\n max_input_len, max_output_len) = build_vocabularies(train_inputs, train_targets, val_inputs, val_targets)\n\n# Prepare data for model input\ntrain_enc_in, train_dec_in, train_dec_out = prepare_data(\n    train_inputs, max_input_len, input_vocab, train_targets, max_output_len, output_vocab)\nval_enc_in, val_dec_in, val_dec_out = prepare_data(\n    val_inputs, max_input_len, input_vocab, val_targets, max_output_len, output_vocab)\ntest_enc_in, test_dec_in, test_dec_out = prepare_data(\n    test_inputs, max_input_len, input_vocab, test_targets, max_output_len, output_vocab)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:45:41.456760Z","iopub.execute_input":"2025-05-21T10:45:41.457485Z","iopub.status.idle":"2025-05-21T10:45:42.540941Z","shell.execute_reply.started":"2025-05-21T10:45:41.457448Z","shell.execute_reply":"2025-05-21T10:45:42.540198Z"}},"outputs":[{"name":"stdout","text":"Training samples: 44204\nValidation samples: 4358\nUnique input tokens: 27\nUnique output tokens: 66\nMax input length: 20\nMax output length: 21\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"print(input_vocab)\nprint(output_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:45:44.827956Z","iopub.execute_input":"2025-05-21T10:45:44.828233Z","iopub.status.idle":"2025-05-21T10:45:44.832443Z","shell.execute_reply.started":"2025-05-21T10:45:44.828212Z","shell.execute_reply":"2025-05-21T10:45:44.831631Z"}},"outputs":[{"name":"stdout","text":"{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n{'\\t': 0, '\\n': 1, ' ': 2, 'ँ': 3, 'ं': 4, 'ः': 5, 'अ': 6, 'आ': 7, 'इ': 8, 'ई': 9, 'उ': 10, 'ऊ': 11, 'ऋ': 12, 'ए': 13, 'ऐ': 14, 'ऑ': 15, 'ओ': 16, 'औ': 17, 'क': 18, 'ख': 19, 'ग': 20, 'घ': 21, 'ङ': 22, 'च': 23, 'छ': 24, 'ज': 25, 'झ': 26, 'ञ': 27, 'ट': 28, 'ठ': 29, 'ड': 30, 'ढ': 31, 'ण': 32, 'त': 33, 'थ': 34, 'द': 35, 'ध': 36, 'न': 37, 'प': 38, 'फ': 39, 'ब': 40, 'भ': 41, 'म': 42, 'य': 43, 'र': 44, 'ल': 45, 'व': 46, 'श': 47, 'ष': 48, 'स': 49, 'ह': 50, '़': 51, 'ा': 52, 'ि': 53, 'ी': 54, 'ु': 55, 'ू': 56, 'ृ': 57, 'ॅ': 58, 'े': 59, 'ै': 60, 'ॉ': 61, 'ो': 62, 'ौ': 63, '्': 64, 'ॐ': 65}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1. Dataset Class\nclass TransliterationDataset(Dataset):\n    def __init__(self, enc_inputs, dec_inputs, dec_targets):\n        self.enc_inputs = torch.LongTensor(enc_inputs).to(device)\n        self.dec_inputs = torch.LongTensor(dec_inputs).to(device)\n        self.dec_targets = torch.FloatTensor(dec_targets).to(device)\n\n    def __len__(self):\n        return len(self.enc_inputs)\n\n    def __getitem__(self, idx):\n        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_targets[idx]\n\n# 2. Attention Mechanism\nclass Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        # hidden: (batch_size, dec_hid_dim)\n        # encoder_outputs: (batch_size, src_len, enc_hid_dim)\n        src_len = encoder_outputs.shape[1]\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # (batch_size, src_len, dec_hid_dim)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # (batch_size, src_len, dec_hid_dim)\n        attention = self.v(energy).squeeze(2)  # (batch_size, src_len)\n        return torch.softmax(attention, dim=1)\n\n# 3. Encoder (Modified for Attention)\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout, cell_type, bidirectional=False):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        \n        self.embedding = nn.Embedding(input_size, embedding_size)\n        \n        if cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, \n                              dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        elif cell_type == 'RNN':\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers,\n                             dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        elif cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers,\n                             dropout=dropout, bidirectional=bidirectional, batch_first=True)\n\n    def forward(self, x):\n        embedded = self.embedding(x)  # (batch_size, seq_len, emb_size)\n        outputs, hidden = self.rnn(embedded)\n        return outputs, hidden\n\n# 4. Decoder with Attention\nclass Decoder(nn.Module):\n    def __init__(self, output_size, embedding_size, hidden_size, num_layers, dropout, cell_type, enc_hid_dim):\n        super().__init__()\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.cell_type = cell_type\n        \n        self.embedding = nn.Embedding(output_size, embedding_size)\n        self.attention = Attention(enc_hid_dim * 2, hidden_size)  # Account for bidirectional\n        \n        rnn_input_size = embedding_size + enc_hid_dim * 2  # embedding + context\n        \n        if cell_type == \"LSTM\":\n            self.rnn = nn.LSTM(rnn_input_size, hidden_size, num_layers, \n                              dropout=dropout, batch_first=True)\n        elif cell_type == 'RNN':\n            self.rnn = nn.RNN(rnn_input_size, hidden_size, num_layers,\n                             dropout=dropout, batch_first=True)\n        elif cell_type == 'GRU':\n            self.rnn = nn.GRU(rnn_input_size, hidden_size, num_layers,\n                             dropout=dropout, batch_first=True)\n            \n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden, encoder_outputs):\n        # x: (batch_size, 1)\n        # hidden: (num_layers, batch_size, hidden_size) or tuple for LSTM\n        # encoder_outputs: (batch_size, src_len, enc_hid_dim * 2)\n        \n        embedded = self.embedding(x)  # (batch_size, 1, emb_size)\n        \n        # Get last layer's hidden state\n        if self.cell_type == 'LSTM':\n            h_n = hidden[0][-1]  # (batch_size, hidden_size)\n        else:\n            h_n = hidden[-1]  # (batch_size, hidden_size)\n        \n        # Calculate attention weights\n        attn_weights = self.attention(h_n, encoder_outputs)  # (batch_size, src_len)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, enc_hid_dim*2)\n        \n        # Combine input with context\n        rnn_input = torch.cat([embedded, context], dim=2)  # (batch_size, 1, emb_size + enc_hid_dim*2)\n        \n        # RNN step\n        output, hidden = self.rnn(rnn_input, hidden)\n        prediction = self.fc(output.squeeze(1))  # (batch_size, output_size)\n        \n        return prediction, hidden\n\n# Modified Seq2Seq Class with Proper Hidden State Handling\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, n_enc_layers, n_dec_layers, cell_type):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.cell_type = cell_type\n        self.n_enc_layers = n_enc_layers\n        self.n_dec_layers = n_dec_layers\n        \n        # Bridge layer for bidirectional encoder and layer mismatch\n        if encoder.bidirectional:\n            self.bridge = nn.Linear(2 * encoder.hidden_size, decoder.hidden_size)\n        else:\n            self.bridge = nn.Identity()\n\n    def _adapt_hidden(self, hidden):\n        \"\"\"Process encoder hidden states for decoder initialization\"\"\"\n        if self.cell_type == 'LSTM':\n            h, c = hidden\n            \n            # Handle bidirectional\n            if self.encoder.bidirectional:\n                # Reshape: (n_enc_layers*2, batch, hidden) -> (n_enc_layers, 2, batch, hidden)\n                h = h.view(self.n_enc_layers, 2, -1, self.encoder.hidden_size)\n                c = c.view(self.n_enc_layers, 2, -1, self.encoder.hidden_size)\n                # Combine directions and project\n                h = self.bridge(torch.cat([h[:,0], h[:,1]], dim=-1))\n                c = self.bridge(torch.cat([c[:,0], c[:,1]], dim=-1))\n\n            # Handle layer mismatch by padding with zeros\n            if h.size(0) < self.n_dec_layers:\n                pad_size = self.n_dec_layers - h.size(0)\n                h = torch.cat([\n                    h,\n                    torch.zeros(pad_size, h.size(1), h.size(2)).to(device)\n                ], dim=0)\n                c = torch.cat([\n                    c,\n                    torch.zeros(pad_size, c.size(1), c.size(2)).to(device)\n                ], dim=0)\n            else:\n                h = h[:self.n_dec_layers]\n                c = c[:self.n_dec_layers]\n\n            return (h.contiguous(), c.contiguous())\n        \n        else:  # For GRU/RNN\n            if self.encoder.bidirectional:\n                hidden = hidden.view(self.n_enc_layers, 2, -1, self.encoder.hidden_size)\n                hidden = self.bridge(torch.cat([hidden[:,0], hidden[:,1]], dim=-1))\n            \n            # Handle layer mismatch\n            if hidden.size(0) < self.n_dec_layers:\n                pad_size = self.n_dec_layers - hidden.size(0)\n                hidden = torch.cat([\n                    hidden,\n                    torch.zeros(pad_size, hidden.size(1), hidden.size(2)).to(device)\n                ], dim=0)\n            else:\n                hidden = hidden[:self.n_dec_layers]\n            \n            return hidden.contiguous()\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.size(0)\n        trg_len = trg.size(1)\n        \n        # Encode source sequence\n        encoder_outputs, hidden = self.encoder(src)\n        hidden = self._adapt_hidden(hidden)\n        \n        # Initialize decoder\n        inputs = trg[:, 0].unsqueeze(1)\n        outputs = torch.zeros(batch_size, trg_len, self.decoder.output_size).to(device)\n        \n        for t in range(1, trg_len):\n            output, hidden = self.decoder(inputs, hidden, encoder_outputs)\n            outputs[:, t] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            inputs = trg[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n            \n        return outputs\n\n# Usage Example\n# Initialize model components\nenc_hid_dim = 256\ndec_hid_dim = 256\nbidirectional = True\n\nencoder = Encoder(\n    input_size=len(input_vocab),\n    embedding_size=256,\n    hidden_size=enc_hid_dim,\n    num_layers=2,\n    dropout=0.2,\n    cell_type='LSTM',\n    bidirectional=bidirectional\n).to(device)\n\ndecoder = Decoder(\n    output_size=len(output_vocab),\n    embedding_size=256,\n    hidden_size=dec_hid_dim,\n    num_layers=3,\n    dropout=0.2,\n    cell_type='LSTM',\n    enc_hid_dim=enc_hid_dim\n).to(device)\n\nmodel = Seq2Seq(encoder, decoder, 2, 3, 'LSTM').to(device)\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:45:47.080080Z","iopub.execute_input":"2025-05-21T10:45:47.080332Z","iopub.status.idle":"2025-05-21T10:45:51.323045Z","shell.execute_reply.started":"2025-05-21T10:45:47.080314Z","shell.execute_reply":"2025-05-21T10:45:51.322436Z"}},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(27, 256)\n    (rnn): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(66, 256)\n    (attention): Attention(\n      (attn): Linear(in_features=768, out_features=256, bias=True)\n      (v): Linear(in_features=256, out_features=1, bias=False)\n    )\n    (rnn): LSTM(768, 256, num_layers=3, batch_first=True, dropout=0.2)\n    (fc): Linear(in_features=256, out_features=66, bias=True)\n  )\n  (bridge): Linear(in_features=512, out_features=256, bias=True)\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, clip=1.0):\n    model.train()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    \n    for enc_in, dec_in, dec_out in tqdm(dataloader, desc='Training'):\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(enc_in, dec_in)  # (batch_size, trg_len, output_dim)\n        \n        # Reshape for loss calculation\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)  # (batch*(trg_len-1), output_dim)\n        \n        # Convert one-hot targets to class indices\n        targets = dec_out[:, 1:].argmax(dim=2).reshape(-1)  # (batch*(trg_len-1))\n        \n        # Calculate loss\n        loss = criterion(output, targets)\n        \n        # Backpropagation\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        # Calculate metrics\n        epoch_loss += loss.item()\n        preds = output.argmax(1)\n        correct += (preds == targets).sum().item()\n        total += targets.size(0)\n    \n    return epoch_loss / len(dataloader), correct / total\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for enc_in, dec_in, dec_out in tqdm(dataloader, desc='Evaluating'):\n            output = model(enc_in, dec_in, teacher_forcing_ratio=0.0)\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            targets = dec_out[:, 1:].argmax(dim=2).reshape(-1)\n            \n            loss = criterion(output, targets)\n            \n            epoch_loss += loss.item()\n            preds = output.argmax(1)\n            correct += (preds == targets).sum().item()\n            total += targets.size(0)\n    \n    return epoch_loss / len(dataloader), correct / total\n\n# Initialize model components\nenc_hid_dim = 128\ndec_hid_dim = 256\nbidirectional = True\n\nencoder = Encoder(\n    input_size=len(input_vocab),\n    embedding_size=128,\n    hidden_size=enc_hid_dim,\n    num_layers=2,\n    dropout=0.4,\n    cell_type='RNN',\n    bidirectional=True\n).to(device)\n\ndecoder = Decoder(\n    output_size=len(output_vocab),\n    embedding_size=128,\n    hidden_size=dec_hid_dim,\n    num_layers=3,\n    dropout=0.2,\n    cell_type='RNN',\n    enc_hid_dim=enc_hid_dim\n).to(device)\n\n# 6. Data Loaders\ntrain_dataset = TransliterationDataset(train_enc_in, train_dec_in, train_dec_out)\nval_dataset = TransliterationDataset(val_enc_in, val_dec_in, val_dec_out)\ntest_dataset = TransliterationDataset(test_enc_in, test_dec_in, test_dec_out)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\nmodel = Seq2Seq(encoder, decoder, 2, 3, 'RNN').to(device)\n# Initialize with proper loss function\ncriterion = nn.CrossEntropyLoss(ignore_index=output_vocab[PAD_TOKEN])\n\n\n# Restart training with corrected loss\noptimizer = optim.Adam(model.parameters())\n\nfor epoch in range(1):\n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    val_loss, val_acc = evaluate(model, val_loader, criterion)\n    \n    print(f'Epoch {epoch+1:02}')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:45:54.498317Z","iopub.execute_input":"2025-05-21T10:45:54.498915Z","iopub.status.idle":"2025-05-21T10:46:32.058840Z","shell.execute_reply.started":"2025-05-21T10:45:54.498894Z","shell.execute_reply":"2025-05-21T10:46:32.058087Z"}},"outputs":[{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:33<00:00, 20.47it/s]\nEvaluating: 100%|██████████| 69/69 [00:01<00:00, 60.04it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 01\n\tTrain Loss: 2.183 | Train Acc: 13.02%\n\t Val. Loss: 1.465 |  Val. Acc: 16.86%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import wandb\n\nwandb.login(key='43cc4a6022bf573f56ea92522b3e44bac7bd28b6')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:46:40.000920Z","iopub.execute_input":"2025-05-21T10:46:40.001755Z","iopub.status.idle":"2025-05-21T10:46:48.962718Z","shell.execute_reply.started":"2025-05-21T10:46:40.001731Z","shell.execute_reply":"2025-05-21T10:46:48.962057Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33majay-madkami-iitm\u001b[0m (\u001b[33majay-madkami-iitm-indian-institute-of-technology-mad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Define the sweep configuration\nsweep_config = {\n    'method': 'bayes',  # Random search\n    'metric': {\n        'name': 'accuracy',\n        'goal': 'maximize'  # Objective: Maximize accuracy\n    },\n    'parameters': {\n        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n        'n_enc_layers': {'values': [2]},\n        'n_dec_layers': {'values': [3]},\n        'hidden_layer_size': {'values': [16, 32, 64, 256]},\n        'emb_size': {'values': [16, 32, 64, 256]},\n        'bidirectional': {'values': [True,False]},\n        'dropout': {'values': [0.2, 0.3]},\n        'epochs': {'values': [3]} \n    }\n}\n\n\n# Initialize a new wandb sweep\nsweep_id = wandb.sweep(sweep_config, project=\"Assignment 3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:58.569536Z","iopub.execute_input":"2025-05-21T10:58:58.569796Z","iopub.status.idle":"2025-05-21T10:58:58.871699Z","shell.execute_reply.started":"2025-05-21T10:58:58.569777Z","shell.execute_reply":"2025-05-21T10:58:58.871082Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: i0pmz3un\nSweep URL: https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/sweeps/i0pmz3un\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import wandb\n\n# 6. Data Loaders\ntrain_dataset = TransliterationDataset(train_enc_in, train_dec_in, train_dec_out)\nval_dataset = TransliterationDataset(val_enc_in, val_dec_in, val_dec_out)\ntest_dataset = TransliterationDataset(test_enc_in, test_dec_in, test_dec_out)\n\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ndef main():\n    # Initialize W&B run\n    wandb.init()\n    hyperparams = wandb.config\n    \n    # Model parameters from sweep\n    encoder = Encoder(\n        input_size=len(input_vocab),\n        embedding_size=hyperparams.emb_size,\n        hidden_size=hyperparams.hidden_layer_size,\n        num_layers=hyperparams.n_enc_layers,\n        dropout=hyperparams.dropout,\n        cell_type=hyperparams.cell_type,\n        bidirectional=hyperparams.bidirectional\n    ).to(device)\n    \n    decoder = Decoder(\n        output_size=len(output_vocab),\n        embedding_size=hyperparams.emb_size,\n        hidden_size=hyperparams.hidden_layer_size,\n        num_layers=hyperparams.n_dec_layers,\n        dropout=hyperparams.dropout,\n        cell_type=hyperparams.cell_type,\n        enc_hid_dim=hyperparams.hidden_layer_size\n    ).to(device)\n\n\n    model = Seq2Seq(encoder, decoder, hyperparams.n_enc_layers, hyperparams.n_dec_layers, hyperparams.cell_type).to(device)\n    \n    # Initialize with proper loss function\n    criterion = nn.CrossEntropyLoss(ignore_index=output_vocab[PAD_TOKEN])\n    # Restart training with corrected loss\n    optimizer = optim.Adam(model.parameters())\n\n    \n    # Training loop\n    for epoch in range(hyperparams.epochs):\n        train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n        val_loss, val_acc = evaluate(model, val_loader, criterion)\n        # Log metrics\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss,\n            \"train_acc\": train_acc,\n            \"val_loss\": val_loss,\n            \"val_acc\": val_acc\n        })\n        \n        print(f'Epoch {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n    \n    # Test evaluation\n    # test_loss, test_acc = evaluate(model, test_loader, criterion)\n    # wandb.log({\"test_loss\": test_loss, \"test_acc\": test_acc})\n    # print(f'\\nFinal Test Accuracy: {test_acc*100:.2f}%')\n\n# Run the sweep\nwandb.agent(sweep_id, function=main, count=1) \nwandb.finish() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:58:59.921824Z","iopub.execute_input":"2025-05-21T10:58:59.922085Z","iopub.status.idle":"2025-05-21T11:01:08.242072Z","shell.execute_reply.started":"2025-05-21T10:58:59.922065Z","shell.execute_reply":"2025-05-21T11:01:08.241355Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: na9y830j with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \temb_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_dec_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tn_enc_layers: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250521_105906-na9y830j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/runs/na9y830j' target=\"_blank\">ruby-sweep-1</a></strong> to <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/sweeps/i0pmz3un' target=\"_blank\">https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/sweeps/i0pmz3un</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203' target=\"_blank\">https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/sweeps/i0pmz3un' target=\"_blank\">https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/sweeps/i0pmz3un</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/runs/na9y830j' target=\"_blank\">https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/runs/na9y830j</a>"},"metadata":{}},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:37<00:00, 18.57it/s]\nEvaluating: 100%|██████████| 69/69 [00:01<00:00, 52.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01\n\tTrain Loss: 2.480 | Train Acc: 10.92%\n\t Val. Loss: 1.802 |  Val. Acc: 14.53%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:37<00:00, 18.61it/s]\nEvaluating: 100%|██████████| 69/69 [00:01<00:00, 52.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02\n\tTrain Loss: 1.497 | Train Acc: 17.55%\n\t Val. Loss: 1.341 |  Val. Acc: 17.68%\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 691/691 [00:36<00:00, 18.73it/s]\nEvaluating: 100%|██████████| 69/69 [00:01<00:00, 52.87it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 03\n\tTrain Loss: 1.221 | Train Acc: 19.50%\n\t Val. Loss: 1.230 |  Val. Acc: 18.48%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_acc</td><td>▁▆█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_acc</td><td>▁▇█</td></tr><tr><td>val_loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_acc</td><td>0.19504</td></tr><tr><td>train_loss</td><td>1.22121</td></tr><tr><td>val_acc</td><td>0.18475</td></tr><tr><td>val_loss</td><td>1.23013</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ruby-sweep-1</strong> at: <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/runs/na9y830j' target=\"_blank\">https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203/runs/na9y830j</a><br> View project at: <a href='https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203' target=\"_blank\">https://wandb.ai/ajay-madkami-iitm-indian-institute-of-technology-mad/Assignment%203</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250521_105906-na9y830j/logs</code>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}