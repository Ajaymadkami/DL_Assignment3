# DL_Assignment3

---

### Overview

The goal of this assignment is to explore sequence-to-sequence learning using Recurrent Neural Networks (RNNs). The work is divided into four main parts:

1. **Modeling Seq2Seq Tasks**: Understand and implement the core idea behind sequence-to-sequence models using RNNs.
2. **Cell Comparison**: Experiment with different RNN variants—vanilla RNN, LSTM, and GRU—and compare their performance.
3. **Attention Mechanism**: Dive into attention-based models to see how they address the limitations of basic seq2seq architectures, especially for longer sequences.
4. **Visualization**: Visualize how different components in the RNN-based model interact during training and inference, providing deeper insight into model behavior.

This assignment provides hands-on experience with building, training, and analyzing neural models for tasks like transliteration, where input and output sequences differ in structure and length.

---

